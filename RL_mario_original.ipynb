{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1659064553324,
     "user": {
      "displayName": "向井利春",
      "userId": "07606417142891008316"
     },
     "user_tz": -540
    },
    "id": "fs2nTRIt7jGp",
    "outputId": "2d0b9c1a-dd44-4d11-9ef0-88ae2d19f40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 4, 'status': 'small', 'time': 300, 'world': 3, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 175}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124716b7a88>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "import time\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Gymは、Open AIのRL用ツールキット\n",
    "import gym\n",
    "from gym.spaces import Box  # 連続値の状態空間定義用\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# OpenAI Gym用に使うNES エミュレーター\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "#OpenAI Gymのスーパー・マリオ・ブラザーズの環境\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# stage 選択\n",
    "stage = \"3-4\"\n",
    "selected_env = \"SuperMarioBros-\" + stage + \"-v3\"\n",
    "env = gym_super_mario_bros.make(selected_env)\n",
    "\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "# actions for very simple movement\n",
    "# SIMPLE_MOVEMENT = [\n",
    "#     ['NOOP'],\n",
    "#     ['right'],\n",
    "#     ['right', 'A'],\n",
    "#     ['right', 'B'],\n",
    "#     ['right', 'A', 'B'],\n",
    "#     ['A'],\n",
    "#     ['left'],\n",
    "# ]\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "#再現性を保つためにseedを固定\n",
    "seed = 15\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "# 表示テスト\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\") # f文字列（フォーマット文字列）\n",
    "    # [3,240, 256]という配列はそれぞれ、画面の[チャネル数, 高さ、幅]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.axis=('off')\n",
    "plt.imshow(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1659064441047,
     "user": {
      "displayName": "向井利春",
      "userId": "07606417142891008316"
     },
     "user_tz": -540
    },
    "id": "k9GsMA16AtGJ"
   },
   "outputs": [],
   "source": [
    "# ラッパーはエージェントにデータを渡す前にデータを前処理\n",
    "\n",
    "\n",
    "# 状態データサイズ縮小 \n",
    "# この処理をすると、状態サイズ[1, 240, 256]\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]  # =(240,256)\n",
    "        self.observation_space=Box(low=0,high=255,shape=obs_shape,dtype=np.uint8) #状態空間定義\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # [H, W, C] arrayを[C, H, W] tensorに変換\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "# 各observationを正方形の画像にダウンサンプリング\n",
    "# この処理を適用すると、状態のサイズは[1, 84, 84]\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):    # shapeがintならTrue\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:] # カラーチャンネルがあったら付け加える．タプルの'+'は結合．\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0) # GrayScale入力ならsqueeze(0)で次元を減らしている．\n",
    "        return observation\n",
    "  \n",
    "# https://github.com/zakopuro/RL_mario\n",
    "\n",
    "# CustomRewardAndDoneラッパー\n",
    "class CustomRewardAndDoneEnv(gym.Wrapper):\n",
    "    # 初期化\n",
    "    def __init__(self, env, skip):\n",
    "        super(CustomRewardAndDoneEnv, self).__init__(env)\n",
    "        self._skip = skip\n",
    "        self._cur_x = 0.0\n",
    "        self._max_x = 0.0\n",
    "\n",
    "    # リセット\n",
    "    def reset(self, **kwargs):\n",
    "        self._cur_x = 0.0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    # ステップ\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        for i in range(self._skip):\n",
    "            # 各フレームごとの報酬はn番目のフレームに集約\n",
    "            # 報酬を蓄積し、同じ行動を繰り返す\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 報酬の変更\n",
    "        if info['flag_get']:\n",
    "            print(\"GOAL\")\n",
    "            total_reward += 100\n",
    "            \n",
    "        if info['x_pos'] >= self._max_x:\n",
    "            self._max_x = info['x_pos']\n",
    "            total_reward += 10\n",
    "            \n",
    "        if (info['x_pos'] < self._max_x) & (info['x_pos'] - self._max_x > -100):\n",
    "            total_reward += 5\n",
    "                \n",
    "        if info['x_pos'] > self._cur_x:\n",
    "            total_reward += 10\n",
    "\n",
    "        elif info['x_pos'] == self._cur_x:\n",
    "            total_reward -= 2\n",
    "            \n",
    "        else:\n",
    "            total_reward -= 4\n",
    "\n",
    "        self._cur_x = info['x_pos']\n",
    "\n",
    "        total_reward -= 30 * (info['life'] - 1) # if info['life'] < 1:  total_reward -= 30\n",
    "\n",
    "        if info['life'] == 1:\n",
    "            done = True\n",
    "            \n",
    "        total_reward *= 0.2\n",
    "\n",
    "        return state, total_reward, done, info\n",
    "\n",
    "# Wrapper\n",
    "# 4つの連続したグレースケールのフレームを重ね合わせたものになる\n",
    "# マリオが行動を取るたびに環境は状態を返す\n",
    "# サイズ[4, 84, 84]の3次元配列\n",
    "\n",
    "#-----------------------------------------------------\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "# マリオは経験を一時的に記憶\n",
    "\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        # 最適な行動を予測するマリオ用のDNN\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        ε-greedy法で行動を選択\n",
    "\n",
    "        Inputs:\n",
    "            state(LazyFrame):現在の状態における一つの観測オブジェクト(state_dim)次元\n",
    "        Outputs:\n",
    "            action_idx (int): 整数値\n",
    "        \"\"\"\n",
    "        # 探索（EXPLORE）\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # 活用（EXPLOIT）\n",
    "        else:\n",
    "            state = state.__array__()  # ndarray.__array__() コピー作成\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\") # actionを選ぶ\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # exploration_rate減衰\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # ステップ+1\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "class Mario(Mario):\n",
    "    \n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        経験をself.memory (replay buffer)に保存\n",
    "\n",
    "        Inputs:\n",
    "            state (LazyFrame),\n",
    "            next_state (LazyFrame),\n",
    "            action (int),\n",
    "            reward (float),\n",
    "            done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        メモリから経験のバッチを取得します\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch)) \n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "    \n",
    "#-----------------------------------------------------\n",
    "\n",
    "class MarioNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\") # 例外発生\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # 平坦化\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)  # 参照ではなく完全コピー\n",
    "\n",
    "        # Q_target パラメータ固定\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "        \n",
    "#---------------------------------------------------\n",
    "\n",
    "class Mario(Mario):\n",
    "       \n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "class Mario(Mario):\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "class Mario(Mario):\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step < self.burnin: # 初期は学習しない\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "        \n",
    "        # メモリからサンプリング\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # TD Estimate取得\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # TD Target取得\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # 損失をQ_onlineに逆伝播\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ログ保存\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self):\n",
    "\n",
    "        # 指標の履歴\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # record()が呼び出されるたびに追加される移動平均\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "        self.ave_epnum = []\n",
    "\n",
    "        # 現在のエピソードの指標\n",
    "        self.init_episode()\n",
    "        \n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "        return self.curr_ep_reward\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"エピソード終了時の記録\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "        self.ave_epnum.append(episode)  # 追加\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon:.4f} - \" # 小数点4桁まで\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "        )\n",
    "\n",
    "    def plot_graph(self):\n",
    "        plt.plot(self.ave_epnum,  self.moving_avg_ep_rewards, label='rewards')\n",
    "        plt.plot(self.ave_epnum,  self.moving_avg_ep_lengths, label='length')\n",
    "        plt.title('Learning process')\n",
    "        plt.xlabel('Episodes') \n",
    "        plt.legend()\n",
    "        \n",
    "#-----------------------------------------------------       \n",
    "# 設定値\n",
    "        \n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, setting):\n",
    "        super().__init__(state_dim, action_dim)\n",
    "        \n",
    "        self.exploration_rate = setting['EXPLORATION_RATE']\n",
    "        self.exploration_rate_decay = setting['EXPLORATION_RATE_DECAY']\n",
    "        self.exploration_rate_min = setting['EXPLORATION_RATE_MIN']\n",
    "\n",
    "        self.gamma = setting['GAMMA']  # 0.9\n",
    "\n",
    "        self.memory = setting['MEMORY'] #maxlen=100000   # dequeは先頭末尾のアクセスが速いリスト\n",
    "        self.batch_size = setting['BATCH_SIZE']\n",
    "        self.burnin = setting['BURNIN'] # 経験を訓練させるために最低限必要なステップ数 (1e3)\n",
    "        self.learn_every = setting['LEARN_EVERY']  # Q_onlineを更新するタイミングを示すステップ数 (3)\n",
    "        self.sync_every = setting['SYNC_EVERY']  # Q_target & Q_onlineを同期させるタイミングを示すステップ数 (1e3)\n",
    "        \n",
    "        self.curr_step = 0\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=setting['LERNING_RATE']) # (lr=0.0001)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_my(env):\n",
    "    env.reset()\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    # 連続したフレームを1つにまとめ、それを学習モデルに入力\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = CustomRewardAndDoneEnv(env, skip=12) # 報酬とエピソード完了の変更\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GedNw3OTaNCX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:58:25 + 10 時間後に終了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20044\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最長'距離'：x_position=275.0, episode：1, step：33\n",
      "最長'距離'：x_position=771.0, episode：2, step：72\n",
      "最大'報酬'：reward=38.0, episode：2, step：72\n",
      "最長'距離'：x_position=825.0, episode：18, step：420\n",
      "Episode 100 - Step 2064 - Epsilon 0.9122 - Mean Reward -41.304 - Mean Length 20.64 - Mean Loss 0.726 - Mean Q Value -0.759 - \n",
      "最長'距離'：x_position=835.0, episode：134, step：2799\n",
      "Episode 200 - Step 4155 - Epsilon 0.8312 - Mean Reward -44.466 - Mean Length 20.91 - Mean Loss 1.052 - Mean Q Value -3.086 - \n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-3-4-v3\")\n",
    "env = set_my(env)\n",
    "\n",
    "#~~~~~~~ 設定値 ~~~~~~~\n",
    "setting = dict(\n",
    "    EXPLORATION_RATE = 1,\n",
    "    EXPLORATION_RATE_DECAY = 0.9999555,\n",
    "    EXPLORATION_RATE_MIN = 0.075,\n",
    "\n",
    "    GAMMA = 0.9,  # 0.9\n",
    "\n",
    "    MEMORY = deque(maxlen=10000), # maxlen=100000   # dequeは先頭末尾のアクセスが速いリスト\n",
    "            # maxlen が大きいと，\"CUDA out of memory\"が出る．GPUメモリサイズは状況で変化するので調整が必要．\n",
    "            # maxlenを設定した場合、dequeが満杯で要素を追加すると逆側の要素が捨てられる\n",
    "    BATCH_SIZE = 32,\n",
    "    LERNING_RATE = 0.0001,\n",
    "\n",
    "    BURNIN = 1e3, # 経験を訓練させるために最低限必要なステップ数 (1e3)\n",
    "    LEARN_EVERY = 3,  # Q_onlineを更新するタイミングを示すステップ数 (3)\n",
    "    SYNC_EVERY = 1e3,  # Q_target & Q_onlineを同期させるタイミングを示すステップ数 (1e3)\n",
    ")\n",
    "\n",
    "hours = 10 # 何時間学習するか\n",
    "REC_EP = 100 # 何episodeごとに出力するか\n",
    "\n",
    "#~~~~~~~~~~~~~~\n",
    "\n",
    "seconds = 3600*hours # 秒に変換\n",
    "gone_time = 0 # 経過時間\n",
    "start = int(time.time()) # 計測開始\n",
    "\n",
    "e = 0 # episode数 mean_rewardを計算する\n",
    "max_reward = 0 # 最大報酬\n",
    "max_curr_x = 0 # 最長距離\n",
    "curr_ep_reward = 0 # 現在のエピソードの報酬\n",
    "\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%T\"),end=\"\")\n",
    "print(f\" + {int(hours)} 時間後に終了\")\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, setting=setting)\n",
    "logger = MetricLogger()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    e += 1\n",
    "    state = env.reset()\n",
    "\n",
    "    # ゲーム開始！\n",
    "    while True:\n",
    "        env.render() # ローカル環境で画面をウィンドウに表示\n",
    "\n",
    "        # 現在の状態に対するエージェントの行動を決める\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # エージェントが行動を実行\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # 記憶\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 訓練\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # ログ保存\n",
    "        curr_ep_reward = logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 状態の更新\n",
    "        state = next_state\n",
    "\n",
    "        # ゲームが終了したかどうかを確認\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "    gone_time = int(time.time()) - start\n",
    "    \n",
    "    # 最大距離の時モデルをSAVE\n",
    "    if info['x_pos'] > max_curr_x or  info['flag_get']:\n",
    "        max_curr_x = info['x_pos']\n",
    "        print(f\"最長'距離'：x_position={max_curr_x:.1f}, episode：{e}, step：{mario.curr_step}\")\n",
    "        torch.save(mario.net.state_dict(), 'mario_param_new_best.pth')\n",
    "        mario.net.load_state_dict(torch.load('mario_param_new_best.pth'))\n",
    "\n",
    "\n",
    "    # 最大報酬の時モデルをSAVE\n",
    "    if curr_ep_reward > max_reward:\n",
    "        max_reward = curr_ep_reward\n",
    "        print(f\"最大'報酬'：reward={max_reward:.1f}, episode：{e}, step：{mario.curr_step}\")\n",
    "        torch.save(mario.net.state_dict(), 'mario_param_new_best.pth')\n",
    "        mario.net.load_state_dict(torch.load('mario_param_new_best.pth'))\n",
    "\n",
    "    # REC_EPごとに出力\n",
    "    if e % REC_EP == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "\n",
    "    # 1000EPごとに保存\n",
    "    if e % 1000 == 0:\n",
    "        torch.save(mario.net.state_dict(), 'mario_param_new.pth')\n",
    "    \n",
    "    # 経過時間超えたら終了\n",
    "    if gone_time > seconds:\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "torch.save(mario.net.state_dict(), 'mario_param_new.pth')\n",
    "logger.plot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画作成準備\n",
    "# 必要に応じて \n",
    "#  pip install ipywidgets\n",
    "#  conda install -c conda-forge ffmpeg\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "from matplotlib import animation\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    reload(plt)\n",
    "    fig = plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    def animate(i):\n",
    "        img = patch.set_data(frames[i])\n",
    "        return img\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=40)\n",
    "    \n",
    "    anim.save('movie_mario.mp4')  # ビデオファイルをsaveしたい場合\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-3-4-v0\")\n",
    "env = set_my(env)\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "# # Saveしたパラメータの読み込み\n",
    "\n",
    "#~~~~~~~ 設定値 ~~~~~~~\n",
    "setting = dict(\n",
    "    EXPLORATION_RATE = 1,\n",
    "    EXPLORATION_RATE_DECAY = 0.01,\n",
    "    EXPLORATION_RATE_MIN = 0.075,\n",
    "\n",
    "    GAMMA = 0.9,  # 0.9\n",
    "\n",
    "    MEMORY = deque(maxlen=10000), # maxlen=100000   # dequeは先頭末尾のアクセスが速いリスト\n",
    "            # maxlen が大きいと，\"CUDA out of memory\"が出る．GPUメモリサイズは状況で変化するので調整が必要．\n",
    "            # maxlenを設定した場合、dequeが満杯で要素を追加すると逆側の要素が捨てられる\n",
    "    BATCH_SIZE = 32,\n",
    "    LERNING_RATE = 0.0001,\n",
    "\n",
    "    BURNIN = 1e3, # 経験を訓練させるために最低限必要なステップ数 (1e3)\n",
    "    LEARN_EVERY = 3,  # Q_onlineを更新するタイミングを示すステップ数 (3)\n",
    "    SYNC_EVERY = 1e3,  # Q_target & Q_onlineを同期させるタイミングを示すステップ数 (1e3)\n",
    ")\n",
    "\n",
    "hours = 0.1 # 何時間学習するか\n",
    "REC_EP = 1 # 何episodeごとに出力するか\n",
    "\n",
    "#~~~~~~~~~~~~~~\n",
    "\n",
    "seconds = 3600*hours # 秒に変換\n",
    "gone_time = 0 # 経過時間\n",
    "start = int(time.time()) # 計測開始\n",
    "\n",
    "e = 0 # episode数 mean_rewardを計算する\n",
    "max_reward = 0 # 最大報酬\n",
    "max_curr_x = 0 # 最長距離\n",
    "curr_ep_reward = 0 # 現在のエピソードの報酬\n",
    "frames_best = []\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%T\"),end=\"\")\n",
    "print(f\" + {int(hours)} 時間後に終了\")\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, setting=setting)\n",
    "mario.net.load_state_dict(torch.load('mario_param_new_best.pth'))\n",
    "\n",
    "logger = MetricLogger()\n",
    "\n",
    "while True:\n",
    "    frames = [] \n",
    "    e += 1\n",
    "    state = env.reset()\n",
    "\n",
    "    # ゲーム開始！\n",
    "    while True:\n",
    "        frames.append(copy.deepcopy(env.render(mode = 'rgb_array')))\n",
    "\n",
    "        env.render() # ローカル環境で画面をウィンドウに表示\n",
    "\n",
    "        # 現在の状態に対するエージェントの行動を決める\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # エージェントが行動を実行\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # 記憶\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # ログ保存\n",
    "        curr_ep_reward = logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 状態の更新\n",
    "        state = next_state\n",
    "\n",
    "        # ゲームが終了したかどうかを確認\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "    gone_time = int(time.time()) - start\n",
    "    \n",
    "    # 最大距離の時モデルをSAVE\n",
    "    if info['x_pos'] > max_curr_x or  info['flag_get']:\n",
    "        max_curr_x = info['x_pos']\n",
    "        print(f\"最長'距離'：x_position={max_curr_x:.1f}, episode：{e}, step：{mario.curr_step}\")\n",
    "        frames_best = copy.deepcopy(frames)\n",
    "\n",
    "    # 最大報酬の時モデルをSAVE\n",
    "    if curr_ep_reward > max_reward:\n",
    "        max_reward = curr_ep_reward\n",
    "        print(f\"最大'報酬'：reward={max_reward:.1f}, episode：{e}, step：{mario.curr_step}\")\n",
    "        frames_best = copy.deepcopy(frames)\n",
    "\n",
    "    # REC_EPごとに出力\n",
    "    if e % REC_EP == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "    \n",
    "    if (time.time() - start) > seconds or info[\"flag_get\"]:\n",
    "        break\n",
    "    else:\n",
    "        frames = []\n",
    "        \n",
    "env.close()\n",
    "html = display_frames_as_gif(frames_best) ## *** 描画結果をHTMLオブジェクトで取得する  ***\n",
    "html ## *** IPython系でインラインHTML表示 ***\n",
    "# torch.save(mario.net.state_dict(), 'mario_param_new.pth')\n",
    "# logger.plot_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM4qtYRIKTA6chJvYFlJtHy",
   "collapsed_sections": [],
   "name": "Gymで強化学習-2.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "35ddba3fb8da44750b4c30b9efd25625b3b9cb98c18559df1e61ce7c09b550f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
